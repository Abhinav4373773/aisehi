{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"Moq5gpYclmB0","executionInfo":{"status":"ok","timestamp":1716448030416,"user_tz":-330,"elapsed":12,"user":{"displayName":"Hello World","userId":"11339240047337825704"}}},"outputs":[],"source":["from google.colab import drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WxCTQiDnl8cA"},"outputs":[],"source":["drive.mount('/content/drive',force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1-XPDU9stxZz"},"outputs":[],"source":["!pip install tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nVvJ2IqjwWRR"},"outputs":[],"source":["!pip install transformers==4.31.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CGENIztKmI9u"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sCWskpsWBet0"},"outputs":[],"source":["max_length = 128  # Maximum length of input sentence to the model.\n","batch_size = 32\n","epochs = 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VZ3djOzdBiYi"},"outputs":[],"source":["labels = [\"contradiction\", \"entailment\", \"neutral\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Am3zxYGBmtz"},"outputs":[],"source":["class BertSemanticDataGenerator(tf.keras.utils.Sequence):\n","    \"\"\"Generates batches of data.\n","\n","    Args:\n","        sentence_pairs: Array of premise and hypothesis input sentences.\n","        labels: Array of labels.\n","        batch_size: Integer batch size.\n","        shuffle: boolean, whether to shuffle the data.\n","        include_targets: boolean, whether to incude the labels.\n","\n","    Returns:\n","        Tuples `([input_ids, attention_mask, `token_type_ids], labels)`\n","        (or just `[input_ids, attention_mask, `token_type_ids]`\n","         if `include_targets=False`)\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        sentence_pairs,\n","        labels,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        include_targets=True,\n","    ):\n","        self.sentence_pairs = sentence_pairs\n","        self.labels = labels\n","        self.shuffle = shuffle\n","        self.batch_size = batch_size\n","        self.include_targets = include_targets\n","        # Load our BERT Tokenizer to encode the text.\n","        # We will use base-base-uncased pretrained model.\n","        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n","            \"bert-base-uncased\", do_lower_case=True\n","        )\n","        self.indexes = np.arange(len(self.sentence_pairs))\n","        self.on_epoch_end()\n","\n","    def __len__(self):\n","        # Denotes the number of batches per epoch.\n","        return len(self.sentence_pairs) // self.batch_size\n","\n","    def __getitem__(self, idx):\n","        # Retrieves the batch of index.\n","        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n","        sentence_pairs = self.sentence_pairs[indexes]\n","\n","        # With BERT tokenizer's batch_encode_plus batch of both the sentences are\n","        # encoded together and separated by [SEP] token.\n","        encoded = self.tokenizer.batch_encode_plus(\n","            sentence_pairs.tolist(),\n","            add_special_tokens=True,\n","            max_length=max_length,\n","            return_attention_mask=True,\n","            return_token_type_ids=True,\n","            pad_to_max_length=True,\n","            return_tensors=\"tf\",\n","        )\n","\n","        # Convert batch of encoded features to numpy array.\n","        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n","        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n","        token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n","\n","        # Set to true if data generator is used for training/validation.\n","        if self.include_targets:\n","            labels = np.array(self.labels[indexes], dtype=\"int32\")\n","            return [input_ids, attention_masks, token_type_ids], labels\n","        else:\n","            return [input_ids, attention_masks, token_type_ids]\n","\n","    def on_epoch_end(self):\n","        # Shuffle indexes after each epoch if shuffle is set to True.\n","        if self.shuffle:\n","            np.random.RandomState(42).shuffle(self.indexes)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lvFs7ASWBsLT"},"outputs":[],"source":["strategy = tf.distribute.MirroredStrategy()\n","\n","with strategy.scope():\n","    # Encoded token ids from BERT tokenizer.\n","    input_ids = tf.keras.layers.Input(\n","        shape=(max_length,), dtype=tf.int32, name=\"input_ids\"\n","    )\n","    # Attention masks indicates to the model which tokens should be attended to.\n","    attention_masks = tf.keras.layers.Input(\n","        shape=(max_length,), dtype=tf.int32, name=\"attention_masks\"\n","    )\n","    # Token type ids are binary masks identifying different sequences in the model.\n","    token_type_ids = tf.keras.layers.Input(\n","        shape=(max_length,), dtype=tf.int32, name=\"token_type_ids\"\n","    )\n","    # Loading pretrained BERT model.\n","    bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")\n","    # Freeze the BERT model to reuse the pretrained features without modifying them.\n","    bert_model.trainable = False\n","\n","    bert_output = bert_model.bert(\n","        input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n","    )\n","    sequence_output = bert_output.last_hidden_state\n","    pooled_output = bert_output.pooler_output\n","    # Add trainable layers on top of frozen layers to adapt the pretrained features on the new data.\n","    bi_lstm = tf.keras.layers.Bidirectional(\n","        tf.keras.layers.LSTM(64, return_sequences=True)\n","    )(sequence_output)\n","    # Applying hybrid pooling approach to bi_lstm sequence output.\n","    avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n","    max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)\n","    concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n","    dropout = tf.keras.layers.Dropout(0.3)(concat)\n","    output = tf.keras.layers.Dense(3, activation=\"softmax\")(dropout)\n","    model = tf.keras.models.Model(\n","        inputs=[input_ids, attention_masks, token_type_ids], outputs=output\n","    )\n","\n","    model.compile(\n","        optimizer=tf.keras.optimizers.Adam(),\n","        loss=\"categorical_crossentropy\",\n","        metrics=[\"acc\"],\n","    )\n","\n","\n","print(f\"Strategy: {strategy}\")\n","model.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YQ8Q9RMwFX9l"},"outputs":[],"source":["from tensorflow import keras\n","\n","loaded_model = tf.keras.models.load_model('/content/drive/MyDrive/Bert/loaded_model.h5')\n","\n","def check_similarity(sentence1, sentence2):\n","    sentence_pairs = np.array([[str(sentence1), str(sentence2)]])\n","    test_data = BertSemanticDataGenerator(\n","        sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False,\n","    )\n","\n","    proba = loaded_model.predict(test_data[0])[0]\n","    idx = np.argmax(proba)\n","    proba = f\"{proba[idx]: .2f}%\"\n","    pred = labels[idx]\n","    return pred, proba\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ty84eE4fiIy"},"outputs":[],"source":["import re\n","import warnings\n","import pandas as pd\n","\n","def calculate_similarity_scores(evaluator_content, student_content):\n","    def create_dictionary(content):\n","        # Remove numeric indices before \"Question\" and \"Answer\" words, preserving the colon\n","        content_without_indices = re.sub(r'(\\b\\d+\\.\\s*Question:)', 'Question:', content)\n","        content_without_indices = re.sub(r'(\\b\\d+\\.\\s*Answer:)', 'Answer:', content_without_indices)\n","\n","        # Tokenize and print each word with punctuation\n","        words = re.findall(r'\\b\\w+\\b|[.,;!?:]', content_without_indices)\n","\n","        sentence = \"\"\n","        my_dic = {}\n","        n = 0\n","\n","        for word in words:\n","            if word == \"Answer\" and words[n + 1] == \":\":\n","                key = sentence.strip()\n","                sentence = \"\"\n","            if word == \"Question\" and n > 4 and words[n + 1] == \":\":\n","                my_dic[key] = sentence.strip()\n","                sentence = \"\"\n","            sentence += word\n","            sentence += \" \"\n","            n += 1\n","\n","        # Check for the last question-answer pair\n","        if sentence.strip() and key:\n","            my_dic[key] = sentence.strip()\n","\n","        return my_dic\n","\n","    eval_dic = create_dictionary(evaluator_content)\n","    std_dic = create_dictionary(student_content)\n","\n","    data = {'Evaluator': [], 'Student': [], 'per_mark': [], 'Similarity': []}\n","    total_entailment = 0\n","    total_neutral = 0\n","    total_contradiction = 0\n","\n","    for fn in eval_dic.keys():\n","        sentence_1 = eval_dic[fn]\n","        sentence_2 = std_dic[fn]\n","        result = check_similarity(sentence_1, sentence_2)\n","\n","        # Extracting percentage value from the tuple\n","        percentage = float(result[1].strip('%'))\n","\n","        data['Student'].append(sentence_2)\n","        data['Evaluator'].append(sentence_1)\n","        data['per_mark'].append(percentage)\n","        data['Similarity'].append(result[0])\n","\n","        # Update total scores based on result type\n","        if result[0] == 'entailment':\n","            total_entailment += percentage\n","        elif result[0] == 'neutral':\n","            total_neutral += percentage / 3\n","        elif result[0] == 'contradiction':\n","            total_contradiction += percentage\n","\n","    # Calculate average scores\n","    num_entailment = len([x for x in data['Similarity'] if x == 'entailment'])\n","    num_neutral = len([x for x in data['Similarity'] if x == 'neutral'])\n","    num_contradiction = len([x for x in data['Similarity'] if x == 'contradiction'])\n","\n","    num = (num_entailment + num_neutral + num_contradiction)\n","    total_contradiction=0\n","    total_accuracy = (total_contradiction + total_entailment + total_neutral) / num\n","\n","    # Add total accuracy to DataFrame\n","    data['Total_Accuracy'] = total_accuracy\n","\n","    df = pd.DataFrame(data)\n","    df.to_csv('output.csv', index=False)\n","    return df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MRSdCmx-G0Ln"},"outputs":[],"source":["\n","# # Example inference\n","# sentence1 = \"Two women are observing something together.\"\n","# sentence2 = \"Two women are standing with their eyes closed.\"\n","# result = check_similarity(sentence1, sentence2)\n","# print(result)\n","\n","# sentence1 = \" The: greenhouse effect is a natural process where specific gases in Earth's atmosphere trap and radiate heat. Solar radiation passes through the atmosphere, warming the Earth's surface. The surface emits infrared radiation, but greenhouse gases like carbon dioxide and methane absorb and re-emit this radiation. This action traps heat, maintaining a temperature suitable for life.\"\n","# sentence2 = \" The: Carbon dioxide (CO2) is harmful to the environment mainly due to its contribution to the greenhouse effect. Excessive CO2, primarily from human activities like burning fossil fuels, intensifies the natural greenhouse effect, leading to global warming and climate change. This results in more frequent and severe weather events, disruptions to ecosystems, and rising sea levels.\"\n","# result = check_similarity(sentence1, sentence2)\n","# print(result)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F4u5X4s0mSpq"},"outputs":[],"source":["# ----from google.colab.output import eval_js\n","#---- print(eval_js(\"google.colab.kernel.proxyPort(5000)\"))"]},{"cell_type":"markdown","metadata":{"id":"UsJLzNpH_U35"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SaedgTR1Fh6J"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ivBt8tUkmXD2"},"outputs":[],"source":["from flask import Flask\n","from flask import request, redirect\n","from flask import render_template"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ug4qljKImg9i"},"outputs":[],"source":["# app = Flask(__name__,template_folder = '/content/drive/MyDrive/Bert/templates')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zJm99dcemkSy"},"outputs":[],"source":["# from IPython.display import clear_output\n","\n","\n","# @app.route('/',methods=['GET','POST'])\n","# def start():\n","#     if request.method =='POST':\n","#         if 'file1' in request.files and 'file2' in request.files:\n","#             evaluator = request.files['file1']\n","#             student = request.files['file2']\n","\n","#             evaluator_path = \"/content/drive/MyDrive/Bert/history/evaluator.txt\"\n","#             student_path = \"/content/drive/MyDrive/Bert/history/student.txt\"\n","\n","#             evaluator.save(evaluator_path)\n","#             student.save(student_path)\n","\n","#             with open(evaluator_path, 'rb') as f1:\n","#                     content1 = f1.read()\n","#                     # evaluator_content = f1.read()\n","#             evaluator = {'evaluator.txt': content1}\n","\n","#             with open(student_path, 'rb') as f2:\n","#                     content2 = f2.read()\n","#                     # student_content = f2.read()\n","#             student = {'student.txt': content2}\n","\n","#             print(evaluator)\n","#             print(student)\n","\n","#             for ky in evaluator.keys():\n","#                   evaluator_content = evaluator[ky].decode('utf-8')\n","#             for ky in student.keys():\n","#                   student_content = student[ky].decode('utf-8')\n","#             clear_output()\n","#             result_df = calculate_similarity_scores(evaluator_content, student_content)\n","#             result_df.to_html('/content/drive/MyDrive/Bert/templates/output.html', index=False)\n","#             print(result_df)\n","\n","#             # result_df.to_csv(file_path, index=False).save(\"/content/drive/MyDrive/Bert/history/output.csv\")\n","#             # result_df.to_csv(file_path, index=False)\n","#             return render_template('output.html')\n","\n","#         else:\n","#             return \"<h3>Sorry we are unable to detect any files. Please Try Again</h3>\"\n","\n","#     return render_template('index.html')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t9e5wGujlRm6"},"outputs":[],"source":["# def output():\n","#   with open('/content/drive/MyDrive/Bert/history/evaluator.txt','rw') as f:\n","#     contents=f.read()\n","#     print(contents)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SK74dO_BVbwG"},"outputs":[],"source":["# !pip install --upgrade google-colab"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u09VWqEhHZAE"},"outputs":[],"source":["\n","# evaluator_path = \"/content/drive/MyDrive/Bert/history/evaluator.txt\"\n","# student_path = \"/content/drive/MyDrive/Bert/history/student.txt\"\n","\n","\n","# with open(evaluator_path, 'rb') as file:\n","#         content = file.read()\n","\n","# evaluator = {'evaluator.txt': content}\n","\n","# with open(student_path, 'rb') as file:\n","#         content = file.read()\n","\n","# student = {'student.txt': content}\n","\n","\n","# for ky in evaluator.keys():\n","#       evaluator_content = evaluator[ky].decode('utf-8')\n","# for ky in student.keys():\n","#       student_content = student[ky].decode('utf-8')\n","\n","# evaluator_content\n","\n","# student_content"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E5muWpe97ihN"},"outputs":[],"source":["# from google.colab import files\n","# evaluator = files.upload()\n","# student1 = files.upload()\n","# student2 = files.upload()\n","# student3 = files.upload()\n","# student4 = files.upload()\n","# for ky in evaluator.keys():\n","#       evaluator_content = evaluator[ky].decode('utf-8')\n","# for ky in student1.keys():\n","#       student_content1 = student1[ky].decode('utf-8')\n","# for ky in student2.keys():\n","#       student_content2= student2[ky].decode('utf-8')\n","# for ky in student3.keys():\n","#       student_content3= student3[ky].decode('utf-8')\n","# for ky in student4.keys():\n","#       student_content4 = student4[ky].decode('utf-8')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kZ8IunwT7wbZ"},"outputs":[],"source":["# result_df = calculate_similarity_scores(evaluator_content, student_content1)\n","# result_df.to_csv('output.csv', index=False)\n","\n","# result_df1 = calculate_similarity_scores(evaluator_content, student_content2)\n","# result_df1.to_csv('output.csv', index=False)\n","\n","# result_df2 = calculate_similarity_scores(evaluator_content, student_content3)\n","# result_df2.to_csv('output.csv', index=False)\n","\n","# result_df3 = calculate_similarity_scores(evaluator_content, student_content4)\n","# result_df3.to_csv('output.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nMavJ0K_8YIT"},"outputs":[],"source":["# result_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4JVayszz8a7e"},"outputs":[],"source":["# result_df1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sKv1Y3H_8dRS"},"outputs":[],"source":["# result_df2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Yx57nk38eBu"},"outputs":[],"source":["# result_df3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KMcT5avwH4j4"},"outputs":[],"source":["# print(\"TensorFlow version:\", tf.__version__)\n","# print(\"Transformers version:\", transformers.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CtlKSGRBcXZ0"},"outputs":[],"source":["# sentence1=\"Answer : The water cycle , or hydrological cycle , consists of several stages . Evaporation occurs when water from the Earth s surface turns into vapor due to heat . Condensation involves the formation of clouds as water vapor cools and turns into liquid . Precipitation occurs when water droplets or ice crystals fall from clouds as rain , snow , sleet , or hail . Runoff involves the flow of water on the Earth s surface back into oceans , rivers , or lakes .\"\n","# sentence2=\"Answer : It is a cycle that made up of water \"\n","# result = check_similarity(sentence1, sentence2)\n","# print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sFE5Em4ZFb2X"},"outputs":[],"source":["from google.colab.output import eval_js\n","print(eval_js(\"google.colab.kernel.proxyPort(5000)\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PLgdin2OGMUl"},"outputs":[],"source":["app = Flask(__name__,template_folder = '/content/drive/MyDrive/Bert/templates',static_folder='/content/drive/MyDrive/Bert/static')\n","\n","from IPython.display import clear_output\n","\n","@app.route('/',methods=['GET','POST'])\n","def start():\n","    if request.method =='POST':\n","        if 'file1' in request.files and 'file2' in request.files:\n","            evaluator = request.files['file1']\n","            student = request.files['file2']\n","\n","            evaluator_path = \"/content/drive/MyDrive/Bert/history/evaluator.txt\"\n","            student_path = \"/content/drive/MyDrive/Bert/history/student.txt\"\n","\n","            evaluator.save(evaluator_path)\n","            student.save(student_path)\n","\n","            with open(evaluator_path, 'rb') as f1:\n","                    content1 = f1.read()\n","                    # evaluator_content = f1.read()\n","            evaluator = {'evaluator.txt': content1}\n","\n","            with open(student_path, 'rb') as f2:\n","                    content2 = f2.read()\n","                    # student_content = f2.read()\n","            student = {'student.txt': content2}\n","\n","            print(evaluator)\n","            print(student)\n","\n","            for ky in evaluator.keys():\n","                  evaluator_content = evaluator[ky].decode('utf-8')\n","            for ky in student.keys():\n","                  student_content = student[ky].decode('utf-8')\n","            clear_output()\n","            result_df = calculate_similarity_scores(evaluator_content, student_content)\n","            result_df.to_html('/content/drive/MyDrive/Bert/templates/output.html', index=False)\n","            print(result_df)\n","\n","            # result_df.to_csv(file_path, index=False).save(\"/content/drive/MyDrive/Bert/history/output.csv\")\n","            # result_df.to_csv(file_path, index=False)\n","            return render_template('output.html')\n","\n","        else:\n","            return \"<h3>Sorry we are unable to detect any files. Please Try Again</h3>\"\n","\n","    return render_template('index.html')\n","\n","\n","@app.route('/show')\n","def shows():\n","    return render_template('index.html')\n","    # return 'this is a product page'\n","\n","\n","@app.route('/about')\n","def about():\n","    return render_template('about.html')\n","\n","if __name__ == \"__main__\":\n","    # !ngrok http 5000\n","    app.run()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}